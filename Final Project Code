# import libraries
import boto3, re, sys, math, json, os, sagemaker, urllib.request
from sagemaker import get_execution_role
import numpy as np                                
import pandas as pd                               
import matplotlib.pyplot as plt                   
from IPython.display import Image                 
from IPython.display import display               
from time import gmtime, strftime                 
from sagemaker.predictor import csv_serializer   

# Define IAM role
role = get_execution_role()
prefix = 'sagemaker/DEMO-xgboost-dm'
containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',
              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',
              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',
              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'} # each region has its XGBoost container
my_region = boto3.session.Session().region_name # set the region of the instance
print("Success - the MySageMakerInstance is in the " + my_region + " region. You will use the " + containers[my_region] + " container for your SageMaker endpoint.")

bucket_name = 'ccfinals3' # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET
s3 = boto3.resource('s3')
try:
    if  my_region == 'us-east-1':
      s3.create_bucket(Bucket=bucket_name)
    else: 
      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })
    print('S3 bucket created successfully')
except Exception as e:
    print('S3 error: ',e)
    
#Data Processing
  #Loading Data
bucket = 'ccfinals3/'
prefix = 'Data/'
df = pd.read_csv('s3://' + bucket + prefix + 'framingham.csv')
df.head()
# Print the dimension of df
pd.DataFrame([[df.shape[0], df.shape[1]]], columns=['# rows', '# columns'])
df.dtypes
df.describe()

  #Identifying the Identifiers
def id_checker(df):
    """
    The identifier checker
    Parameters
    ----------
    df : dataframe
    Returns
    ----------
    The dataframe of identifiers
    """
    
    # Get the identifiers
    df_id = df[[var for var in df.columns 
                if df[var].nunique(dropna=True) == df[var].notnull().sum()]]
    return df_id
    
    # Call id_checker on df
df_id = id_checker(df)

# Print the first 5 rows of df_id
df_id.head()

  #Identifying Missing Values
  def nan_checker(df):
    """
    The NaN checker
    Parameters
    ----------
    df : dataframe
    Returns
    ----------
    The dataframe of variables with NaN, their percentage of NaN and dtype
    """
    
    # Get the dataframe of variables with NaN, their percentage of NaN and dtype
    df_nan = pd.DataFrame([[var, df[var].isnull().sum()*100/len(df[var]), df[var].dtype]
                           for var in df.columns if df[var].isna().sum() > 0],
                          columns=['var', 'percentage', 'dtype'])
    
    # Sort df_nan in accending order of the percentage of NaN
    df_nan = df_nan.sort_values(by='percentage', ascending=False).reset_index(drop=True)
    
    return df_nan
    
    # Call nan_checker on df
df_nan = nan_checker(df)

# Get the variables with missing values, their percentage of missing values and dtype
df_miss = df_nan[df_nan['dtype'] == 'float64'].reset_index(drop=True)

# Print df_miss
df_miss

# Remove rows with missing values from df
df = df.dropna(subset=np.intersect1d(df_miss['var'], df.columns),
                           inplace=False)

# Print the dimension of df
pd.DataFrame([[df.shape[0], df.shape[1]]], columns=['# rows', '# columns'])

def cat_var_checker(df):
    """
    The categorical variable checker
    Parameters
    ----------
    df: the dataframe
   
    Returns
    ----------
    The dataframe of categorical variables and their number of unique value
    """
    
    # Get the dataframe of categorical variables and their number of unique value
    df_cat = pd.DataFrame([[var, df[var].nunique(dropna=False)]
                           for var in df.columns if df[var].dtype == 'object'],
                          columns=['var', 'nunique'])
    
    # Sort df_cat in accending order of the number of unique value
    df_cat = df_cat.sort_values(by='nunique', ascending=False).reset_index(drop=True)
    
    return df_cat
    
    df_id.shape
    
    # Seperate feature variables from target variables
x=df.drop('TenYearCHD', axis=1) #feature columns
y=df.TenYearCHD #target column i.e TenYearCHD
x.head()
x.shape

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

# Print the dimension of x_train
pd.DataFrame([[x_train.shape[0], x_train.shape[1]]], columns=['# rows', '# columns'])

# Print the dimension of x_test
pd.DataFrame([[x_test.shape[0], x_test.shape[1]]], columns=['# rows', '# columns'])

# Print the first 5 rows of x_train
x_train.head()

# Print the first 5 rows of x_test
x_test.head()

y_train.shape
y_test.shape

#Exploratory Data Analysis
